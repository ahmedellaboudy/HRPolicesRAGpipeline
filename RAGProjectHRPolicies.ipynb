{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedellaboudy/HRPolicesRAGpipeline/blob/main/RAGProjectHRPolicies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community pymupdf4llm chromadb sentence-transformers\n"
      ],
      "metadata": {
        "id": "hkB3vpDR43Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "q8b0Kx-R43G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "F_O4b29y43JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Import All Required Modules\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Transformers imports\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "# PyMuPDF import\n",
        "import pymupdf4llm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*generation flags.*\")\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "6dTTk67l43Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 : Extracting markdown\n",
        "def extract_pdf_to_markdown(pdf_path):\n",
        "    md_docs = pymupdf4llm.to_markdown(pdf_path)\n",
        "    print(f\"PDF extracted to markdown successfully!\")\n",
        "    print(f\"Total markdown length: {len(md_docs)} characters\")\n",
        "    return md_docs\n"
      ],
      "metadata": {
        "id": "RcIWYAAT43Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Split by Headers\n",
        "def split_by_headers(md_docs):\n",
        "    \"\"\"Split markdown by headers\"\"\"\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Main Topic\"),\n",
        "        (\"##\", \"Sub-topic\")\n",
        "    ]\n",
        "\n",
        "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    docs = splitter.split_text(md_docs)\n",
        "\n",
        "    print(f\"Found {len(docs)} header-based sections\")\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "77PnMhaz43P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Create Larger Chunks\n",
        "def create_chunks(docs, chunk_size=2000, chunk_overlap=200):\n",
        "    chunk_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    final_chunks = []\n",
        "    for doc in docs:\n",
        "        chunks = chunk_splitter.split_text(doc.page_content)\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            chunk_metadata = doc.metadata.copy()\n",
        "            chunk_metadata['chunk_id'] = idx + 1\n",
        "            chunk_metadata['chunk_size'] = len(chunk)\n",
        "            final_chunks.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
        "\n",
        "    print(f\"Created {len(final_chunks)} chunks with {chunk_size} character size\")\n",
        "    return final_chunks\n"
      ],
      "metadata": {
        "id": "x9E31ghX43SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Clean Text Functions\n",
        "def clean_metadata_value(value):\n",
        "    \"\"\"Helper function to clean metadata strings\"\"\"\n",
        "    if not isinstance(value, str):\n",
        "        return value\n",
        "    value = value.strip()\n",
        "    value = re.sub(r'#+\\s*', '', value)\n",
        "    value = re.sub(r'(\\*\\*|__|\\*)', '', value)\n",
        "    value = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', '', value)\n",
        "    value = re.sub(r'\\s+', ' ', value)\n",
        "    return value.strip()\n",
        "\n",
        "def clean_text(final_chunks):\n",
        "    \"\"\"Clean both text content and metadata\"\"\"\n",
        "    cleaned_final_chunks = []\n",
        "\n",
        "    for doc in final_chunks:\n",
        "        chunk_metadata = {\n",
        "            k: clean_metadata_value(v) for k, v in doc.metadata.items()\n",
        "        }\n",
        "\n",
        "        chunk_content = doc.page_content\n",
        "        cleaned_chunk_content = re.sub(r'#+\\s*', '', chunk_content)\n",
        "        cleaned_chunk_content = re.sub(r'(\\*\\*|__|\\*)', '', cleaned_chunk_content)\n",
        "        cleaned_chunk_content = re.sub(r'^[\\-\\*\\+]\\s*', '', cleaned_chunk_content, flags=re.MULTILINE)\n",
        "\n",
        "        cleaned_final_chunks.append(\n",
        "            Document(page_content=cleaned_chunk_content, metadata=chunk_metadata)\n",
        "        )\n",
        "    return cleaned_final_chunks\n"
      ],
      "metadata": {
        "id": "CK56xOZ343Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Arabic Normalization\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(r'[إأآا]', 'ا', text)\n",
        "    text = re.sub(r'ى', 'ي', text)\n",
        "    text = re.sub(r'ؤ', 'و', text)\n",
        "    text = re.sub(r'ئ', 'ي', text)\n",
        "    text = re.sub(r'ة', 'ه', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "L6vWzRuG43Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Topic Questions Dictionary\n",
        "topic_questions = {\n",
        "    \"dress code\": [\n",
        "        \"What is the company dress code?\",\n",
        "        \"Do I need to wear formal attire?\",\n",
        "        \"Is casual clothing allowed?\",\n",
        "        \"ما هو الزي الرسمي المطلوب في الشركة؟\",\n",
        "        \"هل يسمح بالملابس الكاجوال؟\",\n",
        "        \"ما هي قواعد المظهر العام؟\"\n",
        "    ],\n",
        "    \"working office hours\": [\n",
        "        \"What are the official working hours?\",\n",
        "        \"what is office hours?\",\n",
        "        \"Can I come late or leave early?\",\n",
        "        \"Is there a flexible schedule?\",\n",
        "        \"ما هي ساعات العمل الرسمية؟\",\n",
        "        \"هل يمكنني الحضور متأخراً أو المغادرة مبكراً؟\",\n",
        "        \"هل يوجد جدول عمل مرن؟\"\n",
        "    ],\n",
        "    \"leaves and holidays\": [\n",
        "        \"How many vacation days do I have?\",\n",
        "        \"What is the leave policy?\",\n",
        "        \"Can I take a holiday anytime?\",\n",
        "        \"كم عدد أيام الإجازة المتاحة لي؟\",\n",
        "        \"ما هي سياسة الإجازات؟\",\n",
        "        \"هل يمكنني أخذ عطلة في أي وقت؟\"\n",
        "    ],\n",
        "    \"probation period\": [\n",
        "        \"What is the probation period?\",\n",
        "        \"How long is the trial period for new employees?\",\n",
        "        \"ما هي فترة التجربة للموظفين الجدد؟\",\n",
        "        \"كم تستمر فترة العقد التجريبي؟\"\n",
        "    ],\n",
        "    \"employees termination\": [\n",
        "        \"What is the termination policy?\",\n",
        "        \"How do I submit a resignation?\",\n",
        "        \"What is notice period?\",\n",
        "        \"ما هي سياسة إنهاء الخدمة؟\",\n",
        "        \"كيف أقدم استقالتي؟\",\n",
        "    ],\n",
        "    \"staff payroll\": [\n",
        "        \"When will I receive my salary?\",\n",
        "        \"How does payroll work?\",\n",
        "        \"متى يتم صرف الراتب؟\",\n",
        "        \"كيف يتم احتساب الرواتب؟\",\n",
        "    ],\n",
        "    \"HR Initiatives / Employee Engagement and Development Activities\": [\n",
        "        \"Are there any team-building programs?\",\n",
        "        \"what about employee welfare and activities?\",\n",
        "        \"هل يوجد أنشطة لزيادة تفاعل الموظفين؟\",\n",
        "    ],\n",
        "    \"mobile phone\": [\n",
        "        \"Can I use my mobile phone at work?\",\n",
        "        \"Is social media allowed during office hours?\",\n",
        "        \"هل يمكنني استخدام الهاتف المحمول في العمل؟\",\n",
        "        \"هل يسمح باستخدام وسائل التواصل الاجتماعي أثناء الدوام؟\",\n",
        "    ],\n",
        "    \"absence\": [\n",
        "        \"What should I do if I cannot come to the office?\",\n",
        "        \"Who should I notify if I am absent from work?\",\n",
        "        \"What happens if I am absent for two consecutive days without notice?\",\n",
        "        \"ما هي عواقب الغياب بدون إذن؟\",\n",
        "        \"هل يتم معاقبة الغياب غير المصرح به؟\"\n",
        "    ],\n",
        "    \"staff movement\": [\n",
        "        \"Can I request a transfer to another department?\",\n",
        "        \"What is the relocation policy?\",\n",
        "        \"هل يمكنني طلب نقل إلى قسم آخر؟\",\n",
        "        \"ما هي سياسة النقل أو الانتداب؟\"\n",
        "    ],\n",
        "    \"staff orientation\": [\n",
        "        \"What is included in the orientation program?\",\n",
        "        \"Are company policies explained in orientation?\",\n",
        "        \"ما هو برنامج التعريف بالموظفين الجدد؟\",\n",
        "        \"هل يتم شرح السياسات خلال فترة التعريف؟\"\n",
        "    ],\n",
        "    \"Interim positions and promotions\": [\n",
        "        \"How can I get a promotion?\",\n",
        "        \"What is the policy for reclassification?\",\n",
        "        \"كيف أحصل على ترقية؟\",\n",
        "        \"ما هي سياسة إعادة التصنيف؟\"\n",
        "    ],\n",
        "    \"تقييم الاداء\": [\n",
        "        \"How is performance evaluated?\",\n",
        "        \"When will I get my performance assessment?\",\n",
        "        \"ما هي طريقة تقييم الأداء؟\",\n",
        "        \"متى يتم عمل تقييم للموظف؟\"\n",
        "    ],\n",
        "    \"التدريب والتطوير\": [\n",
        "        \"Are there training programs for employees?\",\n",
        "        \"Can I attend workshops for development?\",\n",
        "        \"هل يوجد برامج تدريبية للموظفين؟\",\n",
        "        \"هل يمكنني حضور ورش عمل للتطوير؟\"\n",
        "    ],\n",
        "    \"مدونه السلوك\": [\n",
        "        \"What is the code of conduct?\",\n",
        "        \"How is workplace harassment handled?\",\n",
        "        \"ما هو السلوك المطلوب في مكان العمل؟\",\n",
        "        \"كيف يتم التعامل مع حالات التحرش؟\"\n",
        "    ],\n",
        "    \"الصحة والسلامة\": [\n",
        "        \"What are the health and safety rules?\",\n",
        "        \"What should I do in case of an emergency?\",\n",
        "        \"ما هي قواعد الصحة والسلامة؟\",\n",
        "        \"ماذا أفعل في حالة الطوارئ؟\"\n",
        "    ],\n",
        "    \"العمل عن بعد\": [\n",
        "        \"Is remote work allowed?\",\n",
        "        \"Can I work from home?\",\n",
        "        \"هل يسمح بالعمل عن بعد؟\",\n",
        "        \"هل يمكنني العمل من المنزل؟\"\n",
        "    ],\n",
        "    \"تسويه الشكاوي\": [\n",
        "        \"How can I file a complaint?\",\n",
        "        \"What is the grievance process?\",\n",
        "        \"كيف أقدم شكوى؟\",\n",
        "        \"ما هي إجراءات النظر في التظلمات؟\"\n",
        "    ],\n",
        "    \"personnel files and documentations\": [\n",
        "        \"What documents are kept in personnel files?\",\n",
        "        \"Are personnel files confidential?\",\n",
        "        \"ما هي المستندات الموجودة في ملفات الموظفين؟\",\n",
        "        \"هل تعتبر ملفات الموظفين سرية؟\"\n",
        "    ],\n",
        "    \"job description\": [\n",
        "        \"Where can I find my job description?\",\n",
        "        \"What are my roles and responsibilities?\",\n",
        "        \"أين أجد الوصف الوظيفي الخاص بي؟\",\n",
        "        \"ما هي مهامي ومسؤولياتي؟\"\n",
        "    ],\n",
        "    \"suggestions\": [\n",
        "        \"How can I submit a suggestion?\",\n",
        "        \"Does the company accept employee ideas?\",\n",
        "        \"كيف يمكنني تقديم اقتراح؟\",\n",
        "        \"هل تقبل الشركة أفكار الموظفين؟\"\n",
        "    ],\n",
        "    \"الانفصال\": [\n",
        "        \"What is the exit process?\",\n",
        "        \"Will there be an exit interview?\",\n",
        "        \"ما هي إجراءات الخروج من الشركة؟\",\n",
        "        \"هل يوجد مقابلة خروج عند الاستقالة؟\"\n",
        "    ],\n",
        "    \"tax deduction\": [\n",
        "        \"What is the professional tax deduction?\",\n",
        "        \"How much tax is deducted from my salary?\",\n",
        "        \"ما هو الخصم الضريبي من المرتب؟\",\n",
        "        \"كم قيمة الضريبة التي تخصم من راتبي؟\",\n",
        "    ],\n",
        "    \"late_coming\": [\n",
        "        \"What is considered late coming at work?\",\n",
        "        \"What should I do if I know I will be late?\",\n",
        "        \"هل يعتبر التأخير عن مواعيد العمل مخالفة؟\",\n",
        "        \"كم دقيقة تأخير مسموحة قبل اعتباره تأخير رسمي؟\",\n",
        "    ],\n",
        "    \"office_rooms\": [\n",
        "        \"what should i do after leaving office room\",\n",
        "        \"كيف يمكنني حجز غرفة مكتب للاجتماعات؟\",\n",
        "        \"هل توجد غرف مشتركة للموظفين؟\",\n",
        "    ],\n",
        "    \"traveling_allowance\": [\n",
        "        \"What is the traveling allowance policy?\",\n",
        "        \"what should i mention in the allowence form?\",\n",
        "        \"ما هي سياسة بدل السفر؟\",\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "Ld6pOCMe43Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Setup Vector Store and Smart Search\n",
        "def setup_vectorstore(final_cleaned_chunks):\n",
        "    model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        final_cleaned_chunks,\n",
        "        embedding_model,\n",
        "        persist_directory=None,\n",
        "        ids=[str(i) for i in range(len(final_cleaned_chunks))]\n",
        "    )\n",
        "\n",
        "    vectorstore.persist()\n",
        "    print(\"Chroma database with 2000-char chunks stored successfully!\")\n",
        "\n",
        "    return vectorstore, embedding_model\n",
        "\n",
        "def prepare_topic_embeddings(topic_questions, embedding_model):\n",
        "    \"\"\"Prepare topic question embeddings\"\"\"\n",
        "    topic_questions_embedded = {\n",
        "        topic: [embedding_model.embed_query(normalize_arabic(q.lower().strip(\"?!.\"))) for q in questions]\n",
        "        for topic, questions in topic_questions.items()\n",
        "    }\n",
        "    return topic_questions_embedded\n",
        "\n",
        "def get_most_relevant_topic_names(query, topic_questions_embedded, embedding_model):\n",
        "    \"\"\"Get most relevant topic for the query\"\"\"\n",
        "    normalized_query = normalize_arabic(query.lower().strip(\"?!.\"))\n",
        "    encoded_query = embedding_model.embed_query(normalized_query)\n",
        "    topic_similarity_scores = defaultdict(lambda: -1.0)\n",
        "\n",
        "    for topic, questions_list in topic_questions_embedded.items():\n",
        "        for question in questions_list:\n",
        "            cos_sim = cosine_similarity([encoded_query], [question])[0][0]\n",
        "            cos_sim = round(cos_sim, 2)\n",
        "            if cos_sim > topic_similarity_scores[topic]:\n",
        "                topic_similarity_scores[topic] = cos_sim\n",
        "\n",
        "    topic_with_highest_score = max(topic_similarity_scores, key=topic_similarity_scores.get)\n",
        "    return topic_with_highest_score\n",
        "\n",
        "def smart_search(query, vectorstore, topic_questions_embedded, embedding_model, k=5, debug=False):\n",
        "    \"\"\"Updated smart search function that returns (document, score) tuples\"\"\"\n",
        "    normalized_query = normalize_arabic(query.lower().strip(\"?!.\"))\n",
        "    results = vectorstore.similarity_search_with_score(normalized_query, k=k)\n",
        "    topic_with_highest_score = get_most_relevant_topic_names(query, topic_questions_embedded, embedding_model)\n",
        "\n",
        "    if debug:\n",
        "        print(f'Most related topic: {topic_with_highest_score}')\n",
        "        print('++++++++++++++++++++++++++++++++++++++++++++++++')\n",
        "\n",
        "    encoded_topic_with_highest_score = embedding_model.embed_query(\n",
        "        normalize_arabic(topic_with_highest_score.lower().strip(\"?!.\"))\n",
        "    )\n",
        "\n",
        "    # Return chunks with the most related topic along with their scores\n",
        "    related_chunks = []\n",
        "    for doc, score in results:\n",
        "        if debug:\n",
        "            print(f'Document score: {score}')\n",
        "            print(f'Document topic: {doc.metadata.get(\"Sub-topic\", \"\")}')\n",
        "\n",
        "        doc_topic = doc.metadata.get('Sub-topic', '')\n",
        "        if doc_topic:  # Only process if topic exists\n",
        "            encoded_topic = embedding_model.embed_query(normalize_arabic(doc_topic.lower().strip(\"?!.\")))\n",
        "            cos_sim = cosine_similarity([encoded_topic], [encoded_topic_with_highest_score])[0][0]\n",
        "            cos_sim = round(cos_sim, 2)\n",
        "\n",
        "            if debug:\n",
        "                print(f'Cosine similarity with topic: {cos_sim}')\n",
        "\n",
        "            if cos_sim > 0.4:\n",
        "                related_chunks.append((doc, score))\n",
        "\n",
        "        if debug:\n",
        "            print('----------------------------------------------')\n",
        "\n",
        "    # If no topic-related chunks found, return top results\n",
        "    if not related_chunks:\n",
        "        related_chunks = results[:min(2, len(results))]\n",
        "\n",
        "    return related_chunks\n"
      ],
      "metadata": {
        "id": "Yz_PkqDz5IP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Model Setup and LLM Pipeline\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    \"\"\"Custom stopping criteria for text generation\"\"\"\n",
        "    def __init__(self, stop_sequences, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_token_ids = []\n",
        "        for seq in stop_sequences:\n",
        "            token_ids = self.tokenizer.encode(seq, add_special_tokens=False)\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.stop_token_ids.append(torch.tensor(token_ids, device=device))\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in self.stop_token_ids:\n",
        "            if len(input_ids[0]) >= len(stop_ids):\n",
        "                if torch.all(input_ids[0][-len(stop_ids):] == stop_ids):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "def setup_model_and_chains():\n",
        "    \"\"\"Setup the language model and LangChain chains\"\"\"\n",
        "    print(\"Loading the model and tokenizer...\")\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Load model based on available hardware\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.bfloat16\n",
        "            ).to(\"cuda\")\n",
        "            print(\"Model loaded with bfloat16 on GPU.\")\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float32\n",
        "            )\n",
        "            print(\"Model loaded with float32 on CPU.\")\n",
        "    except Exception as e:\n",
        "        print(f\"BF16 load failed. Falling back to float16. Error: {e}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16\n",
        "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Model loaded with float16.\")\n",
        "\n",
        "    # Define multilingual stop words\n",
        "    stop_words = [\"User:\", \"\\nUser:\", \"المستخدم:\", \"\\nالمستخدم:\", \"Human:\", \"\\nHuman:\", \"Question:\", \"\\nQuestion:\"]\n",
        "    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_words, tokenizer)])\n",
        "\n",
        "    # Create the pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Wrap the pipeline in LangChain LLM\n",
        "    llm = HuggingFacePipeline(\n",
        "        pipeline=pipe,\n",
        "        model_kwargs={\"stopping_criteria\": stopping_criteria}\n",
        "    )\n",
        "\n",
        "    # Create prompt templates\n",
        "    prompt_template_en = PromptTemplate(\n",
        "        input_variables=[\"context\", \"input\"],\n",
        "        template=\"\"\"Answer the question directly using only the provided context. Be concise and specific.\n",
        "If the answer is not in the context, say: \"I don't know based on the policy.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    )\n",
        "\n",
        "    prompt_template_ar = PromptTemplate(\n",
        "        input_variables=[\"context\", \"input\"],\n",
        "        template=\"\"\"أجب عن السؤال باختصار باستخدام المعلومات التالية فقط. إذا لم تجد الإجابة في السياق، قل: \"لا أعلم بناءً على السياسة.\"\n",
        "\n",
        "السياق: {context}\n",
        "\n",
        "السؤال: {input}\n",
        "\n",
        "الجواب:\"\"\"\n",
        "    )\n",
        "\n",
        "    # Create LLM chains\n",
        "    llm_chain_en = LLMChain(llm=llm, prompt=prompt_template_en, verbose=False)\n",
        "    llm_chain_ar = LLMChain(llm=llm, prompt=prompt_template_ar, verbose=False)\n",
        "\n",
        "    print(\"Model and chains ready!\")\n",
        "\n",
        "    return llm_chain_en, llm_chain_ar\n"
      ],
      "metadata": {
        "id": "6OETSzd-5ISY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 10 : Helper functions\n",
        "def is_arabic(text: str) -> bool:\n",
        "    return any('\\u0600' <= ch <= '\\u06FF' for ch in text)\n",
        "\n",
        "def is_vague(query: str) -> bool:\n",
        "    \"\"\"Check if query is too vague\"\"\"\n",
        "    q_lower = query.strip().lower()\n",
        "    if len(q_lower.split()) <= 2:\n",
        "        return True\n",
        "    vague_terms = {\"policy\", \"leave\", \"hours\", \"benefits\", \"ساعات\", \"اجازة\", \"سياسة\", \"work\"}\n",
        "    return q_lower in vague_terms\n",
        "\n",
        "def retrieve_context_with_smart_search(query: str, vectorstore, topic_questions_embedded, embedding_model, k: int = 2, max_chars: int = 4000) -> str:\n",
        "    \"\"\"Retrieve context using smart search\"\"\"\n",
        "    try:\n",
        "        results = smart_search(query, vectorstore, topic_questions_embedded, embedding_model, k=k)\n",
        "        if not results:\n",
        "            return \"\"\n",
        "\n",
        "        # Extract documents from results\n",
        "        docs = [result[0] for result in results]\n",
        "\n",
        "        # Combine content\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "        # Limit context length\n",
        "        if len(context) > max_chars:\n",
        "            context = context[:max_chars].rsplit('\\n', 1)[0]\n",
        "\n",
        "        return context\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in smart search: {e}\")\n",
        "        # Fallback to regular similarity search\n",
        "        docs = vectorstore.similarity_search(query, k=k)\n",
        "        if not docs:\n",
        "            return \"\"\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "        if len(context) > max_chars:\n",
        "            context = context[:max_chars].rsplit('\\n', 1)[0]\n",
        "        return context\n",
        "\n",
        "def clean_response(response: str, is_arabic_query: bool = False) -> str:\n",
        "    \"\"\"Clean up the model response\"\"\"\n",
        "    response = response.strip()\n",
        "\n",
        "    # Remove common unwanted phrases\n",
        "    unwanted_phrases = [\n",
        "        \"Based on the context\", \"If you have any more questions\", \"please let me know\",\n",
        "        \"The answer should be\", \"There is no need to repeat\", \"However, since\",\n",
        "        \"إذا كنت تريد\", \"يمكنك كتابة المزيد\", \"كما هو موضح أدناه\",\n",
        "        \"إذا كنت تريد توضيح\", \"في حالة عدم وجود\"\n",
        "    ]\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = [s.strip() for s in response.split('.') if s.strip()]\n",
        "\n",
        "    clean_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Skip sentences with unwanted phrases\n",
        "        if not any(phrase in sentence for phrase in unwanted_phrases):\n",
        "            if is_arabic_query:\n",
        "                # Clean Arabic text\n",
        "                cleaned_sentence = ''.join(char for char in sentence if\n",
        "                    char.isspace() or\n",
        "                    '\\u0600' <= char <= '\\u06FF' or  # Arabic\n",
        "                    '\\u0030' <= char <= '\\u0039' or  # Numbers\n",
        "                    char in '.:،؛؟!-()/')\n",
        "                if cleaned_sentence.strip():\n",
        "                    clean_sentences.append(cleaned_sentence.strip())\n",
        "            else:\n",
        "                clean_sentences.append(sentence)\n",
        "\n",
        "    if clean_sentences:\n",
        "        response = '. '.join(clean_sentences)\n",
        "        if not response.endswith('.'):\n",
        "            response += '.'\n",
        "\n",
        "    # If response is too long, take first 2 sentences\n",
        "    sentences = response.split('.')\n",
        "    if len(sentences) > 3:\n",
        "        response = '. '.join(sentences[:2]) + '.'\n",
        "\n",
        "    return response\n",
        "\n",
        "def debug_print_retrieval(query, vectorstore, topic_questions_embedded, embedding_model, k=5):\n",
        "    \"\"\"Debug function to print retrieval details\"\"\"\n",
        "    results = smart_search(query, vectorstore, topic_questions_embedded, embedding_model, k=k, debug=True)\n",
        "    print(\"---- Retrieved chunks (debug) ----\")\n",
        "    for i, (doc, score) in enumerate(results, 1):\n",
        "        print(f\"[{i}] Score: {score:.6f}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print(doc.page_content[:300] + (\"...\" if len(doc.page_content) > 300 else \"\"))\n",
        "        print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "K_86U3hH5IUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Main Execution Pipeline\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    md_docs = extract_pdf_to_markdown(\"HR_policies 1.pdf\")\n",
        "\n",
        "    docs = split_by_headers(md_docs)\n",
        "\n",
        "    final_chunks = create_chunks(docs)\n",
        "\n",
        "    final_cleaned_chunks = clean_text(final_chunks)\n",
        "\n",
        "    vectorstore, embedding_model = setup_vectorstore(final_cleaned_chunks)\n",
        "\n",
        "    topic_questions_embedded = prepare_topic_embeddings(topic_questions, embedding_model)\n",
        "\n",
        "    llm_chain_en, llm_chain_ar = setup_model_and_chains()\n",
        "\n",
        "    print(\"\\n=== HR Assistant Chat (English/Arabic) ===\")\n",
        "    print(\"Type 'exit', 'quit', or 'خروج' to end the conversation.\")\n",
        "    print(\"Type 'debug' followed by your query to see retrieval details.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        # Exit conditions\n",
        "        if user_input.lower() in {\"exit\", \"quit\", \"خروج\"}:\n",
        "            print(\"Bot: Goodbye! / وداعاً!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        # Debug mode\n",
        "        if user_input.lower().startswith(\"debug \"):\n",
        "            debug_query = user_input[6:].strip()\n",
        "            if debug_query:\n",
        "                debug_print_retrieval(debug_query, vectorstore, topic_questions_embedded, embedding_model)\n",
        "            continue\n",
        "\n",
        "        # Handle vague queries\n",
        "        if is_vague(user_input):\n",
        "            if is_arabic(user_input):\n",
        "                print(\"Bot: هل يمكنك توضيح أي سياسة تقصد؟ مثل 'الإجازة المرضية' أو 'ساعات العمل'\")\n",
        "            else:\n",
        "                print(\"Bot: Could you be more specific? For example: 'sick leave policy', 'working hours', 'annual leave'\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Retrieve context using smart search\n",
        "            context = retrieve_context_with_smart_search(\n",
        "                user_input, vectorstore, topic_questions_embedded, embedding_model, k=2, max_chars=4000\n",
        "            )\n",
        "\n",
        "            if not context:\n",
        "                if is_arabic(user_input):\n",
        "                    print(\"Bot: لا أعلم بناءً على السياسة.\")\n",
        "                else:\n",
        "                    print(\"Bot: I don't know based on the policy.\")\n",
        "                continue\n",
        "\n",
        "            # Choose the appropriate chain\n",
        "            is_arabic_query = is_arabic(user_input)\n",
        "\n",
        "            if is_arabic_query:\n",
        "                response = llm_chain_ar.invoke({\"input\": user_input, \"context\": context})\n",
        "            else:\n",
        "                response = llm_chain_en.invoke({\"input\": user_input, \"context\": context})\n",
        "\n",
        "            # Extract response text\n",
        "            if isinstance(response, dict):\n",
        "                bot_response = response.get(\"text\", str(response))\n",
        "            else:\n",
        "                bot_response = str(response)\n",
        "\n",
        "            # Clean the response\n",
        "            bot_response = clean_response(bot_response, is_arabic_query)\n",
        "\n",
        "            # Handle empty responses\n",
        "            if not bot_response or bot_response.lower() in {\"\", \".\", \"answer:\", \"الجواب:\"}:\n",
        "                if is_arabic_query:\n",
        "                    bot_response = \"لا أعلم بناءً على السياسة.\"\n",
        "                else:\n",
        "                    bot_response = \"I don't know based on the policy.\"\n",
        "\n",
        "            print(f\"Bot: {bot_response}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Bot: Sorry, an error occurred: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"Chat session ended.\")"
      ],
      "metadata": {
        "id": "YapN3VBi5SGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mAn6ygf45SI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}